---
# Title, summary, and position in the list
# linktitle: ""
summary: ""
weight: 690

# Basic metadata
title: "RNN Resource"
date: 2020-08-03
draft: false
type: docs # page type
authors: ["admin"]
tags: ["Deep Learning", "RNN", "Resource"]
categories: ["NLP"]
toc: true # Show table of contents?

# Advanced metadata
profile: false  # Show author profile?

reading_time: true # Show estimated reading time?
summary: ""
share: false  # Show social sharing links?
featured: true

comments: false  # Show comments?
disable_comment: true
commentable: false  # Allow visitors to comment? Supported by the Page, Post, and Docs content types.

editable: false  # Allow visitors to edit the page? Supported by the Page, Post, and Docs content types.

# Optional header image (relative to `static/img/` folder).
header:
  caption: ""
  image: ""

# Menu
menu: 
    deep-learning:
        parent: rnn
        weight: 9

---

## RNN

- Tutorials
  - [Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9) :fire::thumbsup:
    - Video Tutorial: {{< youtube LHXXI4-IEns >}}

</br>

- Implementation
  - [min-char-rnn](https://gist.github.com/karpathy/d4dee566867f8291f086) 

- Application of RNN:
  - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

## LSTM

- Tutorials

  - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) :fire::thumbsup:
  - [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) :fire::thumbsup:
    - Video Tutorial: {{< youtube 8HyCNIVRbSU >}}

  </br>

  - [如何从RNN起步，一步一步通俗理解LSTM](https://blog.csdn.net/v_JULY_v/article/details/89894058)

  - [通俗有趣地解释RNN和LSTM](https://www.zhihu.com/question/314002073/answer/613515841) :fire::thumbsup:

    

- Implementation

  - [Deriving LSTM Gradient for Backpropagation](https://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/)




- Deeper understanding
  - [Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)

